import tensorflow as tf
import numpy as np
from tensorflow.keras import backend as K
from datasets import load_dataset
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras import models
from tqdm import tqdm
from tensorflow.keras.losses import sparse_categorical_crossentropy


# Load the dataset from HuggingFace
train_dataset = load_dataset("dllllb/rosbank-churn", "train")
train_data = train_dataset['train']
df_train = pd.DataFrame(train_data)

def preprocess_rosbank_data(df):
    """
    Preprocess the Rosbank dataset for TensorFlow: Normalize numerical features, one-hot encode categorical ones, 
    and group transactions into sequences (e.g., for each user or session).
    
    Args:
        df (pd.DataFrame): The raw transaction data.
        
    Returns:
        X_sequences (list of np.array): List of padded sequences of features.
        y_sequences (np.array): Corresponding labels.
        sequence_lengths (list): Sequence lengths for each user's transactions.
    """
    # Handle missing data if any
    df = df.dropna(subset=['amount', 'MCC', 'target_flag', 'TRDATETIME'])

    # Sort by user ID (cl_id) and timestamp (TRDATETIME) to ensure the transaction order for each user
    df['TRDATETIME'] = pd.to_datetime(df['TRDATETIME'], format='%d%b%y:%H:%M:%S')
    df = df.sort_values(by=['TRDATETIME'])  

    # Normalize the numerical columns ('amount' and 'MCC')
    scaler = StandardScaler()
    df[['amount', 'MCC']] = scaler.fit_transform(df[['amount', 'MCC']])

    # Group transactions by user (assuming 'cl_id' as user ID) and treat each user's transactions as a sequence
    grouped = df.groupby('cl_id')

    # Convert the features and target to lists of arrays (for sequence input to RNN)
    X_sequences = []
    y_sequences = []
    sequence_lengths = []

    for _, group in grouped:
        X_sequences.append(group[['amount', 'MCC']].values)  # Features
        y_sequences.append(group["target_flag"].values[-1])  # Target (last transaction)
        sequence_lengths.append(len(group))  # Sequence length (number of transactions)

    # Pad sequences to ensure uniform length across the batch
    padded_X_sequences = pad_sequences(X_sequences, padding='post', dtype='float32', value=0)

    y_sequences = np.array(y_sequences)

    return padded_X_sequences, y_sequences, sequence_lengths

def fgsm_attack(model, X, y, epsilon=0.1, num_steps=30):
    """
    Performs the Fast Gradient Sign Method (FGSM) attack on the model over multiple steps.
    
    Args:
        model: The trained TensorFlow model.
        X: The original input data (e.g., images or features).
        y: The true labels for the input data.
        epsilon: The magnitude of the perturbation (how much to perturb the input).
        num_steps: Number of steps for perturbation (30 steps as per the user requirement).
        
    Returns:
        X_adv: The adversarial examples generated by the FGSM attack.
    """
    # Make the input tensor of type float32 to allow gradient computation
    X_tensor = tf.convert_to_tensor(X, dtype=tf.float32)
    y_tensor = tf.convert_to_tensor(y, dtype=tf.int32)

    # Apply the attack over multiple steps
    for _ in tqdm(range(num_steps)):
        with tf.GradientTape() as tape:
            tape.watch(X_tensor)  # Watch the input for gradient computation
            
            # Get the model's prediction and calculate the loss
            predictions = model(X_tensor)
            loss = sparse_categorical_crossentropy(y_tensor, predictions)
        
        # Calculate the gradients of the loss with respect to the input image
        gradients = tape.gradient(loss, X_tensor)
        
        # Get the sign of the gradients to create the perturbation
        perturbations = tf.sign(gradients)  # Sign of the gradients
        
        # Generate adversarial examples by adding the perturbation to the input
        X_tensor = X_tensor + epsilon * perturbations
    
        # Clip the values to ensure they stay within a valid range (if necessary)
        X_tensor = tf.clip_by_value(X_tensor, 0.0, 1.0)  # Clip between 0 and 1 for images
    
    return X_tensor.numpy()  # Convert tensor to numpy array for further use

X_train_tensor, y_train_tensor, train_lengths = preprocess_rosbank_data(df_train)

# Split the data into training and test sets
X_train_tensor, X_test_tensor, y_train_tensor, y_test_tensor = train_test_split(X_train_tensor, y_train_tensor, test_size=0.35, random_state=42)

X_test_tensor = X_test_tensor[:50]
y_test_tensor = y_test_tensor[:50]
# Create and train the model
input_shape = X_train_tensor.shape[1:]  # Shape of the input data
model = models.load_model('GRU_model.h5')  # Replace with your saved model path

# Evaluate the model on original test data
original_loss, original_accuracy = model.evaluate(X_test_tensor, y_test_tensor)
print(f"Original Test Accuracy: {original_accuracy * 100:.2f}%")

# Perform FGSM attack on the test data
epsilon = 0.1 
num_steps = 30
X_adv = fgsm_attack(model, X_test_tensor, y_test_tensor, epsilon, num_steps)

# Evaluate the model on adversarial examples
adversarial_loss, adversarial_accuracy = model.evaluate(X_adv, y_test_tensor)
print(f"Adversarial Test Accuracy (epsilon={epsilon}): {adversarial_accuracy * 100:.2f}%")
